{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a20764-686f-42d8-9911-081e9ccc7194",
   "metadata": {},
   "source": [
    "blog on data cleaning: https://monkeylearn.com/blog/text-cleaning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77634b23-e3bd-40d7-acd6-dac4906f5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install flair\n",
    "# ! pip install emoji\n",
    "# ! pip install contextualSpellCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368e61ab-990e-4987-88bd-629e183e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "# import string\n",
    "import emoji\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# # nltk.download('wordnet')\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('average_perception_tagger')\n",
    "# # nltk.download('wordnet')\n",
    "# # nltk.download('words') # if its needed\n",
    "\n",
    "# import spacy\n",
    "# from spacy.tokenizer import Tokenizer\n",
    "# from spacy.lang.en import English \n",
    "# nlp = English()\n",
    "\n",
    "# from collections import Counter, defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dd9347-2e7a-438a-b754-2d1042a83532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6329ae33-5aae-4395-956a-25e2de780eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1551734038204923904</td>\n",
       "      <td>2022-07-26 00:59:59+00:00</td>\n",
       "      <td>$2.7 billion for climate change (slashing carb...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1551734021591269377</td>\n",
       "      <td>2022-07-26 00:59:55+00:00</td>\n",
       "      <td>@nathaliejacoby1 Climate change. The rise in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1551734013815029761</td>\n",
       "      <td>2022-07-26 00:59:53+00:00</td>\n",
       "      <td>@JacobsVegasLife @LasVegasLocally This is a ch...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1551733993740980224</td>\n",
       "      <td>2022-07-26 00:59:48+00:00</td>\n",
       "      <td>Climate Change and Energy Minister Chris Bowen...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1551733979316887554</td>\n",
       "      <td>2022-07-26 00:59:45+00:00</td>\n",
       "      <td>@Thebs15800518 At 5:30, @SecGranHolm tries to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                 created_at  \\\n",
       "0  1551734038204923904  2022-07-26 00:59:59+00:00   \n",
       "1  1551734021591269377  2022-07-26 00:59:55+00:00   \n",
       "2  1551734013815029761  2022-07-26 00:59:53+00:00   \n",
       "3  1551733993740980224  2022-07-26 00:59:48+00:00   \n",
       "4  1551733979316887554  2022-07-26 00:59:45+00:00   \n",
       "\n",
       "                                               tweet  like_count  quote_count  \\\n",
       "0  $2.7 billion for climate change (slashing carb...          15            1   \n",
       "1  @nathaliejacoby1 Climate change. The rise in t...           2            0   \n",
       "2  @JacobsVegasLife @LasVegasLocally This is a ch...           8            0   \n",
       "3  Climate Change and Energy Minister Chris Bowen...          18            0   \n",
       "4  @Thebs15800518 At 5:30, @SecGranHolm tries to ...           0            0   \n",
       "\n",
       "   reply_count  retweet_count  \n",
       "0            0              6  \n",
       "1            0              0  \n",
       "2            1              0  \n",
       "3            8              5  \n",
       "4            0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = pd.read_csv('data.csv')\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11be8792-d274-45bf-a8b7-a56cae6b9010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146069, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = tweet_data.drop_duplicates()\n",
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278b45cf-26d1-49ee-ae9b-82669060f03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146069, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = tweet_data[tweet_data['tweet']!='tweet']\n",
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11734e-f7cb-4e5d-af34-37c7148c8b9b",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b18647-daf9-4bcb-b5c4-e4c42ef5ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = tweet_data.copy()\n",
    "\n",
    "# make all tweet text lower case \n",
    "df_clean['clean'] = df_clean.tweet.str.lower()\n",
    "\n",
    "# extract hashtags from tweet before removing punctuation \n",
    "df_clean['hashtags'] = df_clean.clean.apply(lambda x: [word for word in x.split(' ') if word.startswith('#')])\n",
    "\n",
    "# remove new line \n",
    "df_clean.clean = df_clean.clean.apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "\n",
    "# remove mentions of other twitter users\n",
    "df_clean.clean = df_clean.clean.apply(lambda x: re.sub(r'&amp\\S+', '', x))\n",
    "\n",
    "# Remove punctuation, URLS and @mentions\n",
    "df_clean.clean = df_clean.clean.apply(lambda x: re.sub(r\"(@\\S+)|(#\\S+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x))\n",
    "\n",
    "# remove excess white space \n",
    "df_clean.clean = df_clean.clean.apply(lambda x: x.strip())\n",
    "df_clean.clean = df_clean.clean.apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# remove any emojis\n",
    "df_clean.clean = df_clean.clean.apply(lambda x: emoji.replace_emoji(x, replace=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe6ba4-229d-46bb-af45-db9752fab636",
   "metadata": {},
   "source": [
    "### Check for spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b46897-0e2d-4b9d-8da7-57480122180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contextualSpellCheck\n",
    "# # contextualSpellCheck.add_to_pipe(nlp)\n",
    "\n",
    "# print(df_clean.iloc[2].tweet)\n",
    "# doc = nlp(df_clean.iloc[2].clean)\n",
    "# print('original: ', doc)\n",
    "# print('corrected: ', doc._.outcome_spellCheck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c6b60-0ba8-430c-b663-4fbe64c7f30e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745dca58-8656-43b7-9cca-c9618b93150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27 billion for climate change slashing carbon ...\n",
       "1    climate change the rise in temperature will be...\n",
       "2    this is a chilling podcast about what could ha...\n",
       "3    climate change and energy minister chris bowen...\n",
       "4    at 530 tries to hide the fact that began signi...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026aa14-9ef2-4291-94fb-b559160889dd",
   "metadata": {},
   "source": [
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70beaf-8195-4013-b239-34627badf17c",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad33e70-5a55-4e38-8cfd-10775ce390cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 billion for climate change slashing carbon emissions 37 billion for cops tell me again why republicans arent in love with joe biden i mean at least when it comes to this\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27 billion climate change slashing carbon emissions 37 billion cops tell republicans arent love joe biden mean comes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "print(df_clean.iloc[0]['clean'])\n",
    "df_clean['clean_spacy'] = df_clean.clean.apply(lambda x: ' '.join([word for word in x.split(' ') if word not in STOP_WORDS]))\n",
    "df_clean.iloc[0]['clean_spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26a20e-2876-4409-95a8-1256a354ced8",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dcb97ec-37d8-4d53-93fd-cb14e0e8621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text_to_join'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(['text_to_join'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c43b4a-08bc-4f82-b371-6cc88a8f4dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 billion climate change slashing carbon emissions 37 billion cops tell republicans arent love joe biden mean comes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27 billion climate change slash carbon emission 37 billion cop tell republicans be not love joe biden mean come'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_clean.clean_spacy[0])\n",
    "df_clean['clean_spacy'] = df_clean.clean_spacy.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_space]))\n",
    "df_clean.clean_spacy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c7b60-3bd0-4178-9263-913a37dc229e",
   "metadata": {},
   "source": [
    "Remove stopwords again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bfde6bb-9d32-46e6-9246-c7865a15e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 billion climate change slash carbon emission 37 billion cop tell republicans be not love joe biden mean come\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'27 billion climate change slash carbon emission 37 billion cop tell republicans love joe biden mean come'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_clean.iloc[0]['clean_spacy'])\n",
    "df_clean.clean_spacy = df_clean.clean_spacy.apply(lambda x: ' '.join([word for word in x.split(' ') if word not in STOP_WORDS]))\n",
    "df_clean.iloc[0]['clean_spacy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcadbb-eb4c-4de8-a1ef-646714144ac3",
   "metadata": {},
   "source": [
    "### Frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1e4f7a4-fa11-4b41-92fe-95331fe02831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate 138594\n",
      "change 134530\n",
      "people 12956\n",
      "like 9826\n",
      "s 9427\n",
      "year 9109\n",
      "world 8522\n",
      "need 8429\n",
      "global 7459\n",
      "think 7360\n",
      "cause 6973\n",
      "know 6743\n",
      "time 6265\n",
      "bill 6070\n",
      "real 5933\n",
      "fight 5698\n",
      "help 5467\n",
      "new 5389\n",
      "want 5374\n",
      "right 5361\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for sent in df_clean.clean_spacy:\n",
    "    sent = sent.split(' ')\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "        \n",
    "for word in sorted(word_freq, key=word_freq.get, reverse=True)[:20]:\n",
    "    print(word, word_freq[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fdf3786-041c-43e2-baf7-261618637f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('data_lg_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7bab3-b30b-4a8c-a934-95a0e3676f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
