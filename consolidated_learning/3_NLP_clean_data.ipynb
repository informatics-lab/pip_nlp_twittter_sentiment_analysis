{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b438db5-4112-4004-8052-b7fd5557f4b6",
   "metadata": {},
   "source": [
    "# NLP Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a20764-686f-42d8-9911-081e9ccc7194",
   "metadata": {},
   "source": [
    "blog on data cleaning: https://monkeylearn.com/blog/text-cleaning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368e61ab-990e-4987-88bd-629e183e584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c486cc3-0ea3-4de6-b899-3caf9523886f",
   "metadata": {},
   "source": [
    "To work with SpaCy, we have to download the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dd9347-2e7a-438a-b754-2d1042a83532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://metoffice.jfrog.io/metoffice/api/pypi/pypi/simple\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: jinja2 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (65.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /net/data/users/hbrown/conda/envs/nlp/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6329ae33-5aae-4395-956a-25e2de780eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1551734038204923904</td>\n",
       "      <td>2022-07-26 00:59:59+00:00</td>\n",
       "      <td>$2.7 billion for climate change (slashing carb...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1551734021591269377</td>\n",
       "      <td>2022-07-26 00:59:55+00:00</td>\n",
       "      <td>@nathaliejacoby1 Climate change. The rise in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1551734013815029761</td>\n",
       "      <td>2022-07-26 00:59:53+00:00</td>\n",
       "      <td>@JacobsVegasLife @LasVegasLocally This is a ch...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1551733993740980224</td>\n",
       "      <td>2022-07-26 00:59:48+00:00</td>\n",
       "      <td>Climate Change and Energy Minister Chris Bowen...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1551733979316887554</td>\n",
       "      <td>2022-07-26 00:59:45+00:00</td>\n",
       "      <td>@Thebs15800518 At 5:30, @SecGranHolm tries to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                 created_at  \\\n",
       "0  1551734038204923904  2022-07-26 00:59:59+00:00   \n",
       "1  1551734021591269377  2022-07-26 00:59:55+00:00   \n",
       "2  1551734013815029761  2022-07-26 00:59:53+00:00   \n",
       "3  1551733993740980224  2022-07-26 00:59:48+00:00   \n",
       "4  1551733979316887554  2022-07-26 00:59:45+00:00   \n",
       "\n",
       "                                               tweet  like_count  quote_count  \\\n",
       "0  $2.7 billion for climate change (slashing carb...          15            1   \n",
       "1  @nathaliejacoby1 Climate change. The rise in t...           2            0   \n",
       "2  @JacobsVegasLife @LasVegasLocally This is a ch...           8            0   \n",
       "3  Climate Change and Energy Minister Chris Bowen...          18            0   \n",
       "4  @Thebs15800518 At 5:30, @SecGranHolm tries to ...           0            0   \n",
       "\n",
       "   reply_count  retweet_count  \n",
       "0            0              6  \n",
       "1            0              0  \n",
       "2            1              0  \n",
       "3            8              5  \n",
       "4            0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = pathlib.Path('/project/informatics_lab/pip_nlp_data/')\n",
    "data_fn = 'twitter_data_202207260000_202208010900.csv'\n",
    "tweet_data = pd.read_csv(data_dir / data_fn)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "128932f6-59ed-4abe-ba93-0d931fc2d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167946, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11be8792-d274-45bf-a8b7-a56cae6b9010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146069, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data = tweet_data.drop_duplicates()\n",
    "tweet_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11734e-f7cb-4e5d-af34-37c7148c8b9b",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b23f05-240d-435f-92ca-69cb90c5ad40",
   "metadata": {},
   "source": [
    "First step is to create a new column in the pandas dataframe for cleaned tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe168b9a-c18c-4fb6-a048-b560468fc496",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['clean'] = tweet_data.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e18d54-e4db-4189-b59e-3d6d5ee24ec7",
   "metadata": {},
   "source": [
    "Make all text lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8cf0d05-e4ed-49a8-85b1-d24e73f4a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.clean = tweet_data.clean.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de79b5d-f6be-44a3-aa5e-60ccc211d674",
   "metadata": {},
   "source": [
    "Before we remove punctuation from the text, we identify any hashtags within the tweets and put them in a seperate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f38220-9f5a-4467-9f72-34754afa958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['hashtags'] = tweet_data.clean.apply(lambda x: [word for word in x.split(' ') if word.startswith('#')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152942c4-1557-4355-a420-1c1fde9835b7",
   "metadata": {},
   "source": [
    "Next we remove any punctuation, URLs, mentions of other twitter users and any AMP HTML references. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40887fc3-2c24-4807-b9cf-abffc5fe299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.clean = tweet_data.clean.apply(lambda x: re.sub(r'&amp\\S+', '', x))\n",
    "tweet_data.clean = tweet_data.clean.apply(lambda x: re.sub(r\"(@\\S+)|(#\\S+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ad684-f4c4-4a41-977a-acc112e87c5c",
   "metadata": {},
   "source": [
    "Remove any excess white space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f980595b-9ed7-4035-b394-939f7ca8c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.clean = tweet_data.clean.apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "\n",
    "tweet_data.clean = tweet_data.clean.apply(lambda x: x.strip())\n",
    "tweet_data.clean = tweet_data.clean.apply(lambda x: ' '.join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be6c8c-f079-4b52-a24d-940d17f77172",
   "metadata": {},
   "source": [
    "Finally, we remove any emoji's in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b18647-daf9-4bcb-b5c4-e4c42ef5ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "tweet_data.clean = tweet_data.clean.apply(lambda x: emoji.replace_emoji(x, replace=''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433ceb6d-0bc3-438a-9d83-47639cb512f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:  @nathaliejacoby1 Climate change. The rise in temperature will be bad enough, but the secondary consequences - famine, disease, war, global political and economic instability - are terrifying on an epic scale.\n",
      "\n",
      "After cleaning:  climate change the rise in temperature will be bad enough but the secondary consequences famine disease war global political and economic instability are terrifying on an epic scale\n"
     ]
    }
   ],
   "source": [
    "print('Before cleaning: ', tweet_data.tweet.iloc[1])\n",
    "print('\\nAfter cleaning: ', tweet_data.clean.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ca6a8-bd7e-4a18-be22-3a91fe410f74",
   "metadata": {},
   "source": [
    "Note: that for sentiment analysis, some of the text content that we have cleaned out here, like punctuation and emojis, could be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c6b60-0ba8-430c-b663-4fbe64c7f30e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26a20e-2876-4409-95a8-1256a354ced8",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026aa14-9ef2-4291-94fb-b559160889dd",
   "metadata": {},
   "source": [
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37235cb4-7547-4953-9c10-cba632f650ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:  climate change the rise in temperature will be bad enough but the secondary consequences famine disease war global political and economic instability are terrifying on an epic scale\n",
      "\n",
      "After lemmatization:  climate change the rise in temperature will be bad enough but the secondary consequence famine disease war global political and economic instability be terrify on an epic scale\n"
     ]
    }
   ],
   "source": [
    "print('Before lemmatization: ', tweet_data.clean.iloc[1])\n",
    "tweet_data['clean'] = tweet_data.clean.apply(lambda x: ' '.join([token.lemma_ for token in nlp(x) if not token.is_space]))\n",
    "tweet_data.clean = tweet_data.clean.str.lower()\n",
    "print('\\nAfter lemmatization: ', tweet_data.clean.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c88976-d3ff-45e4-a743-b8f91e96fcdc",
   "metadata": {},
   "source": [
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70beaf-8195-4013-b239-34627badf17c",
   "metadata": {},
   "source": [
    "The SpaCy python package provides a dictionary containing stopwords, things like 'the', 'be', 'a' etc. These words help make text flow, but don't add much information to a sentence. By removing them, we are able to give more focus to important information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dfd9c03-aa16-4aac-a984-2be1edd483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stopwords:  climate change the rise in temperature will be bad enough but the secondary consequence famine disease war global political and economic instability be terrify on an epic scale\n",
      "\n",
      "After removing stopwords:  climate change rise temperature bad secondary consequence famine disease war global political economic instability terrify epic scale\n"
     ]
    }
   ],
   "source": [
    "print('Before removing stopwords: ', tweet_data.clean.iloc[1])\n",
    "tweet_data.clean = tweet_data.clean.apply(lambda x: ' '.join([word for word in x.split(' ') if word not in STOP_WORDS]))\n",
    "print('\\nAfter removing stopwords: ', tweet_data.clean.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcadbb-eb4c-4de8-a1ef-646714144ac3",
   "metadata": {},
   "source": [
    "### Frequently used words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d97c8-0b61-492e-9124-b7e8e4aee908",
   "metadata": {},
   "source": [
    "If we check the frequently used words, we can see that most of these are useful terms that we would expect to appear in relation to climate change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e4f7a4-fa11-4b41-92fe-95331fe02831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate 136999\n",
      "change 130934\n",
      "people 12751\n",
      "like 9753\n",
      "s 9100\n",
      "year 8879\n",
      "need 8398\n",
      "world 8370\n",
      "think 7363\n",
      "global 7254\n",
      "cause 6945\n",
      "know 6698\n",
      "time 6198\n",
      "fight 6097\n",
      "bill 5927\n",
      "real 5761\n",
      "use 5474\n",
      "help 5465\n",
      "want 5451\n",
      "new 5294\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for sent in tweet_data.clean:\n",
    "    sent = sent.split(' ')\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "        \n",
    "for word in sorted(word_freq, key=word_freq.get, reverse=True)[:20]:\n",
    "    print(word, word_freq[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32635b54-73fa-4471-9f14-55cb23455d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>clean</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1551734038204923904</td>\n",
       "      <td>2022-07-26 00:59:59+00:00</td>\n",
       "      <td>$2.7 billion for climate change (slashing carb...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>27 billion climate change slash carbon emissio...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1551734021591269377</td>\n",
       "      <td>2022-07-26 00:59:55+00:00</td>\n",
       "      <td>@nathaliejacoby1 Climate change. The rise in t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>climate change rise temperature bad secondary ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1551734013815029761</td>\n",
       "      <td>2022-07-26 00:59:53+00:00</td>\n",
       "      <td>@JacobsVegasLife @LasVegasLocally This is a ch...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>chill podcast happen salt lake city great salt...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1551733993740980224</td>\n",
       "      <td>2022-07-26 00:59:48+00:00</td>\n",
       "      <td>Climate Change and Energy Minister Chris Bowen...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>climate change energy minister chris bowen hit...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1551733979316887554</td>\n",
       "      <td>2022-07-26 00:59:45+00:00</td>\n",
       "      <td>@Thebs15800518 At 5:30, @SecGranHolm tries to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>530 try hide fact begin sign legislation shut ...</td>\n",
       "      <td>[#biden, #oil, #buildbackbetter]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                 created_at  \\\n",
       "0  1551734038204923904  2022-07-26 00:59:59+00:00   \n",
       "1  1551734021591269377  2022-07-26 00:59:55+00:00   \n",
       "2  1551734013815029761  2022-07-26 00:59:53+00:00   \n",
       "3  1551733993740980224  2022-07-26 00:59:48+00:00   \n",
       "4  1551733979316887554  2022-07-26 00:59:45+00:00   \n",
       "\n",
       "                                               tweet  like_count  quote_count  \\\n",
       "0  $2.7 billion for climate change (slashing carb...          15            1   \n",
       "1  @nathaliejacoby1 Climate change. The rise in t...           2            0   \n",
       "2  @JacobsVegasLife @LasVegasLocally This is a ch...           8            0   \n",
       "3  Climate Change and Energy Minister Chris Bowen...          18            0   \n",
       "4  @Thebs15800518 At 5:30, @SecGranHolm tries to ...           0            0   \n",
       "\n",
       "   reply_count  retweet_count  \\\n",
       "0            0              6   \n",
       "1            0              0   \n",
       "2            1              0   \n",
       "3            8              5   \n",
       "4            0              0   \n",
       "\n",
       "                                               clean  \\\n",
       "0  27 billion climate change slash carbon emissio...   \n",
       "1  climate change rise temperature bad secondary ...   \n",
       "2  chill podcast happen salt lake city great salt...   \n",
       "3  climate change energy minister chris bowen hit...   \n",
       "4  530 try hide fact begin sign legislation shut ...   \n",
       "\n",
       "                           hashtags  \n",
       "0                                []  \n",
       "1                                []  \n",
       "2                                []  \n",
       "3                                []  \n",
       "4  [#biden, #oil, #buildbackbetter]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b18777-43b3-46db-b151-f4e31052d384",
   "metadata": {},
   "source": [
    "### Save clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fdf3786-041c-43e2-baf7-261618637f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = pathlib.Path(os.environ['SCRATCH']) / (data_fn.split('.')[0] + '_clean.csv')\n",
    "tweet_data.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d83fb01-2ac1-4167-a00e-ce3139ab3ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'twitter_data_202207260000_202208010900_clean.csv'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_fn.split('.')[0] + '_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b487141-d5c8-44ae-8c93-4aebf1498a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
